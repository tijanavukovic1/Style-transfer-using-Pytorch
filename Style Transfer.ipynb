{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models,transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VGG pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg=models.vgg19(pretrained=True).features\n",
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image to Tensor, resize it and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_image(img,msize):\n",
    "    im=cv2.imread(img)\n",
    "    if(np.max(im.size)>msize):\n",
    "        shape=msize\n",
    "    else:\n",
    "        shape=max(im.size)\n",
    "        \n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Resize(shape),\n",
    "                                  transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))\n",
    "                                 ])\n",
    "    im=transform(im).unsqueeze(0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load content and style images and trainsform then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "style=load_image('impresionism.jpg',300)\n",
    "content=load_image('bg.jpg',300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image from Tensor to numpy and denormalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(im):\n",
    "    im=im.detach().numpy().squeeze(0)\n",
    "    im=im.transpose(1,2,0)\n",
    "    im=im*np.array((0.485, 0.456, 0.406))+np.array((0.229, 0.224, 0.225))\n",
    "    return im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_show=prepare_img(style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate features of convolutional layers for content and style image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze parameters of pretrained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(im,model):\n",
    "    \n",
    "    features={}\n",
    "    \n",
    "    layers={'0':'conv_1_1',\n",
    "            '5':'conv_2_1',\n",
    "            '10':'conv_3_1',\n",
    "            '19':'conv_4_1',\n",
    "            '21':'conv_4_2',\n",
    "            '28':'conv_5_1'\n",
    "           }\n",
    "    \n",
    "    x=im\n",
    "    \n",
    "    for name,lay in model._modules.items():\n",
    "        x=lay(x)\n",
    "        if(name in layers):\n",
    "            features[layers[name]]=x\n",
    "            \n",
    "    return features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate gram matrix as correlation matrix of features of every layer in features variable in style image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features of convolutional layers for content and style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features=calc_features(content,vgg)\n",
    "style_features=calc_features(style,vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(ft):\n",
    "    \n",
    "    matrix=ft.view(ft.shape[1],ft.shape[2]*ft.shape[3])\n",
    "    gram=torch.mm(matrix,matrix.t())\n",
    "        \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_gram={}\n",
    "\n",
    "for name in style_features:\n",
    "    ft=style_features[name]\n",
    "    style_gram[name]=gram_matrix(ft)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target image which is in first step content image and its parameters are updating during training according to total loss which is $$total loss=\\alpha *content loss+ \\beta*style loss$$\n",
    "Our target image needs to have features of 2nd layer from 4th convolutional stack of VGG-19 like features of content image. \n",
    "Style of the target image needs to be similar as style of style image, so gram matrix of choosen layers needs of the target image needs to be similar to gram matrix of choosen layers of style image.\n",
    "$$content loss=\\frac{1}{2}\\sum(T_c-C_c)^2$$,\n",
    "where $T_c$ are features from target image from selected layers and $C_c$ are features from content image from selected layers.\n",
    "$$style loss=a\\sum{_i} w_i(T_s,i-S_s,i)^2$$,\n",
    "where $T_s,i$ is gram matrix of target image of i-th feature and $S_s,i$ is gram matrix of style image of i-th feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 300, 535])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=content.clone().requires_grad_(True)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up weights for layers in calculating style loss. Range for weight is from 0 to 1 and deeper layers have lower weight than first layers.\n",
    "Setting up content weight and style weight for calculating total loss as\n",
    "$$total loss=$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_w=1\n",
    "style_w=10^6\n",
    "style_weights={'conv_1_1':1,\n",
    "                 'conv_2_1':0.75,\n",
    "                 'conv_3_1':0.5,\n",
    "                 'conv_4_1':0.2,\n",
    "                 'conv_4_2':0.2,\n",
    "                 'conv_5_1':0.2\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training style transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minap\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Total loss 878.6083374023438\n",
      "399 Total loss 379.2178039550781\n",
      "599 Total loss 226.96383666992188\n",
      "799 Total loss 168.22845458984375\n",
      "999 Total loss 141.6485137939453\n",
      "1199 Total loss 127.11407470703125\n",
      "1399 Total loss 117.89907836914062\n",
      "1599 Total loss 111.49398040771484\n",
      "1799 Total loss 106.68290710449219\n",
      "1999 Total loss 102.84024810791016\n",
      "2199 Total loss 99.65975952148438\n",
      "2399 Total loss 96.97737121582031\n",
      "2599 Total loss 94.66635131835938\n",
      "2799 Total loss 92.6919174194336\n",
      "2999 Total loss 91.00465393066406\n",
      "3199 Total loss 89.55375671386719\n",
      "3399 Total loss 88.300537109375\n",
      "3599 Total loss 87.23583984375\n",
      "3799 Total loss 86.2478256225586\n",
      "3999 Total loss 85.41835021972656\n",
      "4199 Total loss 84.66259002685547\n",
      "4399 Total loss 84.08453369140625\n",
      "4599 Total loss 83.47266387939453\n",
      "4799 Total loss 82.93267822265625\n",
      "4999 Total loss 82.5615463256836\n",
      "5199 Total loss 82.1283187866211\n",
      "5399 Total loss 81.76302337646484\n",
      "5599 Total loss 81.35369110107422\n",
      "5799 Total loss 81.05326843261719\n",
      "5999 Total loss 80.79611206054688\n",
      "6199 Total loss 80.51283264160156\n",
      "6399 Total loss 80.3078384399414\n",
      "6599 Total loss 80.05350494384766\n",
      "6799 Total loss 79.88321685791016\n",
      "6999 Total loss 79.74373626708984\n",
      "7199 Total loss 79.44463348388672\n",
      "7399 Total loss 79.335205078125\n",
      "7599 Total loss 79.20203399658203\n",
      "7799 Total loss 79.09220886230469\n",
      "7999 Total loss 78.92820739746094\n",
      "8199 Total loss 78.82192993164062\n"
     ]
    }
   ],
   "source": [
    "optimizer=optim.Adam([target],lr=0.005)\n",
    "n_epochs=10000\n",
    "n=200\n",
    "for i in range(n_epochs):\n",
    "    target_features=calc_features(target,vgg)\n",
    "    cont_loss=torch.mean((content_features['conv_4_2']-target_features['conv_4_2'])**2)\n",
    "    style_loss=0\n",
    "    for name in style_features:\n",
    "        target_gram=gram_matrix(target_features[name])\n",
    "        sl=style_weights[name]*torch.mean((target_gram-style_gram[name])**2)\n",
    "        style_loss+=sl/(target.shape[1]*target.shape[2]*target.shape[3])\n",
    "        \n",
    "    total_loss=style_w*style_loss+content_w*cont_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if((i+1)%n==0):\n",
    "        plt.imsave('style_transfer/image_new'+str(i)+'.jpg',prepare_img(target).clip(0,1))\n",
    "        print(i,\"Total loss\",total_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
